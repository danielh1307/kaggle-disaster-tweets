{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:29:59.937795300Z",
     "start_time": "2024-02-21T21:29:59.920796300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "#input\n",
    "x = torch.tensor([[1,2, 12,34, 56,78, 90,80],\n",
    "                 [12,45, 99,67, 6,23, 77,82],\n",
    "                 [3,24, 6,99, 12,56, 21,22]])\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:32:26.112849Z",
     "start_time": "2024-02-21T20:32:26.101871800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "model1 = nn.Embedding(num_embeddings=100, embedding_dim=7, padding_idx=0)  # map each index in a fixed-size dictionary of size 100 to a dense vector of size 7\n",
    "model2 = nn.LSTM(input_size=7, hidden_size=3, num_layers=1, batch_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:32:26.491788400Z",
     "start_time": "2024-02-21T20:32:26.479766100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 8, 7])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "out1 = model1(x)\n",
    "print(type(out1))  # torch.Tensor\n",
    "print(out1.shape)  # ([3, 8, 7]) --> each element in x is mapped to a vector of size 7\n",
    "out2 = model2(out1)\n",
    "print(type(out2))  # tuple\n",
    "print(type(out2[0]))  # torch.Tensor --> consolidated output of all hidden states in the sequence --> shape [batch_size, seq_len, hidden_size]\n",
    "print(type(out2[1][0]))  # torch.Tensor --> hidden state of the last LSTM unit - the final output\n",
    "print(type(out2[1][1]))  # torch.Tensor --> cell state\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:32:26.825683700Z",
     "start_time": "2024-02-21T20:32:26.816688400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:32:27.820738700Z",
     "start_time": "2024-02-21T20:32:27.790717100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing (tokenization)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "     - -------------------------------------- 0.4/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.6 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 5.3 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.0/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.5/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.1/12.8 MB 7.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 7.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.5/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.0/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.0/12.8 MB 11.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.2/12.8 MB 16.0 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 15.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hammd\\miniconda3\\envs\\kaggle-disaster-tweets-env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "tok = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:33:54.920140100Z",
     "start_time": "2024-02-21T20:33:45.058706100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:35:00.449681900Z",
     "start_time": "2024-02-21T20:35:00.426687500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hugo', '  ', 'hase']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"hugo!!!hase\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:35:50.540734100Z",
     "start_time": "2024-02-21T20:35:50.524739400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in df.iterrows():\n",
    "    counts.update(tokenize(row['text']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:37:20.187831700Z",
     "start_time": "2024-02-21T20:37:18.198168100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words before: 22129\n",
      "num_words after: 7019\n"
     ]
    }
   ],
   "source": [
    "#deleting infrequent words\n",
    "print(\"num_words before:\",len(counts.keys()))\n",
    "for word in list(counts):\n",
    "    if counts[word] < 2:\n",
    "        del counts[word]\n",
    "print(\"num_words after:\",len(counts.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:37:36.466295500Z",
     "start_time": "2024-02-21T20:37:36.453268400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}  # map words to their index\n",
    "words = [\"\", \"UNK\"]  # list of words indexed according to their appearance order\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:37:57.280642200Z",
     "start_time": "2024-02-21T20:37:57.260648200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "def encode_sentence(text, vocab2index, N=70):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(N, dtype=int)\n",
    "    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n",
    "    length = min(N, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return [encoded]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:18:28.848092500Z",
     "start_time": "2024-02-21T21:18:28.819096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target                                            encoded  \n0       1  [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...  \n1       1  [[16, 17, 18, 19, 1, 1, 9, 20, 0, 0, 0, 0, 0, ...  \n2       1  [[15, 21, 22, 23, 9, 24, 25, 26, 9, 4, 27, 1, ...  \n3       1  [[36, 37, 38, 9, 39, 32, 34, 25, 40, 0, 0, 0, ...  \n4       1  [[41, 42, 43, 8, 44, 45, 1, 9, 46, 47, 48, 45,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>[[16, 17, 18, 19, 1, 1, 9, 20, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>[[15, 21, 22, 23, 9, 24, 25, 26, 9, 4, 27, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>[[36, 37, 38, 9, 39, 32, 34, 25, 40, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>[[41, 42, 43, 8, 44, 45, 1, 9, 46, 47, 48, 45,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoded'] = df['text'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:18:30.127598Z",
     "start_time": "2024-02-21T21:18:29.298669800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        print(X[3][0])\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx]\n",
    "        #x_tensor = torch.tensor(self.X[idx][0], dtype=torch.int32)\n",
    "        #return x_tensor, self.y[idx], self.X[idx][1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:35:52.981324900Z",
     "start_time": "2024-02-21T21:35:52.961327700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "X = list(df['encoded'])\n",
    "y = list(df['target'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:35:53.543294600Z",
     "start_time": "2024-02-21T21:35:53.521296900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 127  231    9 1538    9  461   23 3311    9 2204 1623   87  804 2017\n",
      "    9  473    9   50   83  165   83  150  166 5095    1    1    9 5391\n",
      "    9 3857 5396  165   83  150  166 1442    9    1    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[ 220  180    8  236 5976   76    9 4195   85 3399    9 1728  273   25\n",
      "    9 1567    1    9  165   83  150  166 3954    1    9 3959  165   83\n",
      "  150  166    1  150    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "train_ds = ReviewsDataset(X_train, y_train)\n",
    "valid_ds = ReviewsDataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:35:55.141362300Z",
     "start_time": "2024-02-21T21:35:55.107334400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "vocab_size = len(words)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:35:57.401732400Z",
     "start_time": "2024-02-21T21:35:57.363483300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y in train_dl:\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "            y_pred = model(x)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss/total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y in valid_dl:\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        y_hat = model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:51:04.314844700Z",
     "start_time": "2024-02-21T21:51:04.299816600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 2)  # change to 5 for 5 classes\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:51:04.806961800Z",
     "start_time": "2024-02-21T21:51:04.783971500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.685, val loss 0.684, val accuracy 0.570, and val rmse 0.655\n",
      "train loss 0.683, val loss 0.684, val accuracy 0.570, and val rmse 0.655\n",
      "train loss 0.684, val loss 0.684, val accuracy 0.570, and val rmse 0.655\n",
      "train loss 0.683, val loss 0.683, val accuracy 0.570, and val rmse 0.655\n",
      "train loss 0.683, val loss 0.683, val accuracy 0.570, and val rmse 0.655\n",
      "train loss 0.683, val loss 0.683, val accuracy 0.570, and val rmse 0.655\n"
     ]
    }
   ],
   "source": [
    "model_fixed = LSTM_fixed_len(7100, 70, 70)\n",
    "train_model(model_fixed, epochs=30, lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T21:51:28.266176500Z",
     "start_time": "2024-02-21T21:51:05.401061300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
